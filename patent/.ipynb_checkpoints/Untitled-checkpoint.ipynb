{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, math\n",
    "from bs4 import BeautifulSoup\n",
    "from googleapiclient.discovery import build\n",
    "import requests as rs\n",
    "import operator, re\n",
    "import numpy as np\n",
    "from konlpy.tag import Hannanum\n",
    "from google.cloud import translate\n",
    "import os\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_variables():\n",
    "    \"\"\" Initialize global variables \"\"\"\n",
    "    print(\"1. init variables\")\n",
    "\n",
    "    global file_check, translate_client, hannanum\n",
    "\n",
    "    # downloadable file string pattern\n",
    "    file_check = [\"download\", \"down\", \"file\", \"pdf\", \"excel\", \"xlsx\", \"docx\", \"hwp\", \"youtube\"]\n",
    "\n",
    "    # google translate API\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"My Project-a8e42c74ea7e.json\"\n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    # Hannanum pos tagger\n",
    "    hannanum = Hannanum()\n",
    "\n",
    "\n",
    "def init_count_dict():\n",
    "\n",
    "    global company_count_dict\n",
    "    company_count_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_company_list():\n",
    "\n",
    "    company_list = []\n",
    "    with open(\"app/static/data/company_list3.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        company_list.extend(list(np.unique(f.read().splitlines())))\n",
    "\n",
    "    with open(\"app/static/data/abroad_company_list.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        company_list.extend(list(np.unique(f.read().splitlines())))\n",
    "\n",
    "    return company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build an API-specific service\n",
    "def getService():\n",
    "    service = build(\"customsearch\",\n",
    "                    \"v1\",\n",
    "                    developerKey=\"AIzaSyCNLAU_Lunh5aJIo17DlslQvKoQGU7yDjA\")\n",
    "\n",
    "    return service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_derived_query(keyword):\n",
    "    nouns = [word for word, pos in hannanum.pos(keyword) if pos == \"N\"]\n",
    "    syn_dict = {}\n",
    "    query_list = [keyword]\n",
    "\n",
    "    for noun in nouns:\n",
    "        result = translate_client.translate(noun, target_language=\"en\")\n",
    "        if len(result[\"translatedText\"].split(\" \")) > 1:  # 복합 명사 처리 안함\n",
    "            continue\n",
    "        else:\n",
    "            translated_noun = result[\"translatedText\"]\n",
    "            # print(noun, translated_noun)\n",
    "            for syn in wordnet.synsets(translated_noun):\n",
    "                synonyms = []\n",
    "                if syn.pos() == \"n\":\n",
    "                    syn_word = syn.name().split(\".\")[0]\n",
    "                    synonyms.append(syn_word)\n",
    "\n",
    "        syn_dict[noun] = synonyms\n",
    "\n",
    "    if len(syn_dict) > 0:\n",
    "        for noun in syn_dict:\n",
    "            for syn in syn_dict[noun]:\n",
    "                syn_ko = translate_client.translate(syn, target_language=\"ko\")[\"translatedText\"]\n",
    "                query_list.append(keyword.replace(noun, syn_ko))\n",
    "\n",
    "    return query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def google_search(keyword):\n",
    "    service = getService()  # GOOGLE API 연결\n",
    "\n",
    "    response_url = []  # URL list\n",
    "\n",
    "    query_list = get_derived_query(keyword)\n",
    "    print(\"2. retrieve related urls with keyword \", query_list)\n",
    "    for query in query_list:\n",
    "        startIndex = 1  # 시작 인덱스\n",
    "        while (True):\n",
    "            try:\n",
    "                result = service.cse().list(\n",
    "                    q=query,  # 검색 키워드\n",
    "                    cx='001132580745589424302:jbscnf14_dw',  # CSE Key\n",
    "                    lr='lang_ko',  # 검색 언어 (한국어)\n",
    "                    start=startIndex,\n",
    "                    filter=\"0\"\n",
    "                ).execute()\n",
    "\n",
    "                # 검색된 결과가 있을 때\n",
    "                if \"items\" in result:\n",
    "                    for item in result[\"items\"]:\n",
    "                        url = item[\"link\"]\n",
    "                        response_url.append(url)\n",
    "\n",
    "                    # INDEX 이동\n",
    "                    if (len(result[\"items\"]) < 10):  # 결과가 10개 미만이면 STOP\n",
    "                        break\n",
    "                    else:  # 결과가 10개면 이동\n",
    "                        startIndex = startIndex + 10\n",
    "                else:\n",
    "                    # print(\"No more Results\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "\n",
    "    response_url = list(np.unique(response_url))\n",
    "\n",
    "    print(\"The number of all results : \" + str(len(response_url)))\n",
    "\n",
    "    return response_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_url(url_list):\n",
    "    \"\"\"extract urls  for exclusion from all url list\"\"\"\n",
    "    print(\"3. classify page urls\")\n",
    "\n",
    "    download_list = []\n",
    "    html_list = []\n",
    "\n",
    "    for i, url in enumerate(url_list):\n",
    "        # extract downloadable URL including youtube\n",
    "        if (any(ext in url.lower() for ext in file_check)):\n",
    "            download_list.append(url)\n",
    "        else :\n",
    "            html_list.append(url)\n",
    "\n",
    "    print(\"All : \" + str(len(url_list)) + \" ( HTML URL : \" + str(len(html_list)) + \" / Downloadable URL : \" + str(\n",
    "        len(download_list)) + \" )\")\n",
    "    return download_list, html_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def locations_of_substring(string, substring):\n",
    "    \"\"\"Return a list of locations of a substring.\"\"\"\n",
    "\n",
    "    substring_length = len(substring)\n",
    "    def recurse(locations_found, start):\n",
    "        location = string.find(substring, start)\n",
    "        if location != -1:\n",
    "            return recurse(locations_found + [location], location+substring_length)\n",
    "        else:\n",
    "            return locations_found\n",
    "\n",
    "    return recurse([], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_postposition(word):\n",
    "    if word[-3:] == \"에서는\":\n",
    "        return word[:-3]\n",
    "    elif word[-1] in [\"은\", \"는\", \"이\", \"가\", \"의\"]:\n",
    "        return word[:-1]\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_html(url):\n",
    "\n",
    "    try:\n",
    "        response = rs.get(url)\n",
    "        if response.encoding != None:\n",
    "            html = response.text.encode(response.encoding)\n",
    "        else:\n",
    "            html = response.text\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser').body\n",
    "\n",
    "    if soup == None : return None\n",
    "\n",
    "    # remove all script, style\n",
    "    for item in [\"script\", \"style\", \"a\", \"img\"]:\n",
    "        if soup.find(item):\n",
    "            for e in soup.find_all(item):\n",
    "                e.decompose()\n",
    "\n",
    "    # remove div with \"footer\" class\n",
    "    for div in soup.find_all(\"div\", {'class': 'footer'}):\n",
    "        div.decompose()\n",
    "    for div in soup.find_all(\"div\", {'id': 'footer'}):\n",
    "        div.decompose()\n",
    "\n",
    "    # remove \"footer\" element\n",
    "    if soup.find(\"footer\"):\n",
    "        soup.find(\"footer\").decompose()\n",
    "\n",
    "    # extract only text\n",
    "    html_text = soup.get_text().strip()\n",
    "\n",
    "    return html_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. init variables\n",
      "2. retrieve related urls with keyword  ['비침습 혈당 센서', '비침습 혈당 탐지기']\n",
      "The number of all results : 123\n",
      "3. classify page urls\n",
      "All : 123 ( HTML URL : 56 / Downloadable URL : 67 )\n"
     ]
    }
   ],
   "source": [
    "keyword = \"비침습 혈당 센서\"\n",
    "init_variables()\n",
    "company_list = init_company_list()\n",
    "url_list = google_search(keyword)\n",
    "download_list, html_list = classify_url(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL 0. http://biomed.khu.ac.kr/professor/professor_detail.html?seq=3\n",
      "대한\n"
     ]
    }
   ],
   "source": [
    "init_count_dict()\n",
    "\n",
    "for i, url in enumerate(html_list):\n",
    "    # parse html\n",
    "    html_text = parse_html(url)\n",
    "    if html_text == None : continue\n",
    "        \n",
    "    print(\"URL\", str(i) + \". \" + url)\n",
    "    \n",
    "    for company in company_list :\n",
    "        company2 = company.replace(\"(주)\", \"\")\n",
    "        if len(company2) == 1 : continue \n",
    "        if company2 in html_text :\n",
    "            temp_idx = html_text.find(company2)\n",
    "            \n",
    "            if (html_text[temp_idx + len(company2)] in [\"은\", \"는\", \"이\", \"가\", \"의\", \"\\n\", \"\\t\", \"\\r\", \" \", \",\", \".\",\n",
    "                                                               \")\"]) | (\n",
    "                    html_text[temp_idx + len(company2): temp_idx + len(company2) + 2] == \"에서\"):\n",
    "                print(company2)\n",
    "                \n",
    "                break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL 0. https://fic.ulsan.ac.kr/open_content/information/research/detail/major/\n"
     ]
    }
   ],
   "source": [
    "init_count_dict()\n",
    "\n",
    "html_list = [\"https://fic.ulsan.ac.kr/open_content/information/research/detail/major/\"]\n",
    "\n",
    "for i, url in enumerate(html_list):\n",
    "    # if i < 49: continue\n",
    "    # parse html\n",
    "    html_text = parse_html(url)\n",
    "    if html_text == None : continue\n",
    "        \n",
    "    print(\"URL\", str(i) + \". \" + url)\n",
    "        \n",
    "#     # Case 1) 기업명에서 \"㈜\" 포함 시\n",
    "#     found_index_list = []\n",
    "#     for company in company_list:\n",
    "#         company2 = company.replace(\"(주)\", \"\")\n",
    "#         company3 = company.replace(\"(주)\", \"㈜\")\n",
    "        \n",
    "#         if (company in html_text) | (company3 in html_text):\n",
    "#             # 특수문자 \"㈜\" 로 대체\n",
    "#             if company not in html_text:\n",
    "#                 company = company3\n",
    "#             temp_idx = html_text.find(company)\n",
    "#             # 기업명 뒤가 주격조사, 특수문자, 공백 중 하나로 끝나는지 체크\n",
    "#             if (html_text[temp_idx + len(company)] in [\"은\", \"는\", \"이\", \"가\", \"의\", \"\\n\", \"\\t\", \"\\r\", \" \", \",\", \".\",\n",
    "#                                                                \")\"]) | (\n",
    "#                     html_text[temp_idx + len(company): temp_idx + len(company) + 2] == \"에서\"):\n",
    "#                 company = company.replace(\"㈜\", \"(주)\")\n",
    "#                 print(\"Case 1)\", company)\n",
    "#                 if company in company_count_dict:\n",
    "#                     company_count_dict[company][\"count\"] += 1\n",
    "#                     company_count_dict[company][\"url_list\"].append(url)\n",
    "#                 else:\n",
    "#                     company_count_dict[company] = {\"count\": 1, \"url_list\": [url]}\n",
    "#                 found_index_list.append(temp_idx)\n",
    "#             else:\n",
    "#                 pass\n",
    "#                 # print(company, \"is real company name?\")\n",
    "                \n",
    "    '''\n",
    "    Case 3) 기업 DB 사용 X, \"(주)\" 포함 기업명 추가\n",
    "    ''' \n",
    "    try : \n",
    "        cor_words_index = locations_of_substring(html_text, \"(주)\")\n",
    "        cor_words_index += locations_of_substring(html_text, \"㈜\")\n",
    "    except RecursionError as e:\n",
    "        continue\n",
    "\n",
    "    cor_words_index = [i for i in cor_words_index if i not in found_index_list] # Case 1 경우 제외\n",
    "    \n",
    "    if len(cor_words_index) != 0:\n",
    "        for x, idx in enumerate(cor_words_index):\n",
    "            # print(x, \"(\", idx, \").\", end=\" \")\n",
    "            # \"(주)\"가 앞에 붙은 경우\n",
    "            if html_text[idx - 1] in [\" \", \"\\n\", \"\\r\", \"\\t\", \",\", \".\"]:\n",
    "                blank_idx_list = [html_text[idx:].find(c) for c in [\" \", \"\\n\", \"\\r\", \"\\t\", \".\", \",\"]\n",
    "                                            if html_text[idx:].find(c) > 0]\n",
    "                cor_word = html_text[idx:idx + np.min(blank_idx_list)]\n",
    "            # \"(주)\"가 뒤에 붙은 경우\n",
    "            elif html_text[idx + 3] in [\" \", \"\\n\", \"\\r\", \"\\t\", \",\", \".\"]:\n",
    "                blank_idx_list = [html_text[:idx + 3][::-1].find(c) for c in\n",
    "                                            [\" \", \"\\n\", \"\\r\", \"\\t\", \".\", \",\"] if\n",
    "                                            html_text[:idx + 3][::-1].find(c) > 0]\n",
    "                if len(blank_idx_list) == 0:\n",
    "                    cor_word = html_text[:idx + 3][::-1][:][::-1]\n",
    "                else:\n",
    "                    cor_word = html_text[:idx + 3][::-1][:np.min(blank_idx_list)][::-1]\n",
    "            else:\n",
    "                blank_idx_list = [html_text[idx:].find(c) for c in [\" \", \"\\n\", \"\\r\", \"\\t\"] if\n",
    "                                                      html_text[idx:].find(c) > 0]\n",
    "                cor_word = html_text[idx:idx + np.min(blank_idx_list)]\n",
    "\n",
    "            # 특수문자 전체 제거\n",
    "            ju = \"(주)\" if cor_word.find(\"(주)\") != -1 else \"㈜\"\n",
    "            loc_ju = \"f\" if cor_word.find(ju) == 0 else \"b\"\n",
    "            cor_word = cor_word.replace(ju, \"\")\n",
    "            # 괄호 안의 문자 모두 제거\n",
    "            p = re.compile(r'\\([^)]*\\)')\n",
    "            cor_word = re.sub(p, \"\", cor_word)\n",
    "            cor_word = re.sub('[?|$|.,-=|!()•]', '', cor_word).strip()\n",
    "            cor_word = ju + cor_word if loc_ju == \"f\" else cor_word + ju\n",
    "\n",
    "            # 기업명이 문자 한 개 이하면 패스\n",
    "            if len(cor_word.replace(ju, \"\")) <= 1:\n",
    "                print(cor_word, \" name too short\")\n",
    "                continue\n",
    "\n",
    "            cor_word = cor_word.replace(\"㈜\", \"(주)\")\n",
    "            cor_word = remove_postposition(cor_word)\n",
    "            # 기업명 DB에 이미 존재할 경우 패스\n",
    "            exist = False\n",
    "            # 주격조사 제거 시 고려\n",
    "            # cor_word2 = remove_postposition(cor_word)\n",
    "            for ext in company_list:\n",
    "                if cor_word in ext:\n",
    "                    if cor_word == ext:\n",
    "                        # cor_word = cor_word2\n",
    "                        print(\"Case 3)\", cor_word, \"already exists in the DB\")\n",
    "                        exist = True\n",
    "                        break\n",
    "\n",
    "            if exist == False:\n",
    "                print(\"Case 3)\", cor_word)\n",
    "                company_list.append(cor_word)\n",
    "                company_count_dict[cor_word] = {\"count\": 1, \"url_list\": [url]}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def locations_of_substring(string, substring):\n",
    "    \"\"\"Return a list of locations of a substring.\"\"\"\n",
    "\n",
    "    substring_length = len(substring)\n",
    "    def recurse(locations_found, start):\n",
    "        location = string.find(substring, start)\n",
    "        if location != -1:\n",
    "            return recurse(locations_found + [location], location+substring_length)\n",
    "        else:\n",
    "            return locations_found\n",
    "\n",
    "    return recurse([], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "company_count_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
