{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math, re\n",
    "from bs4 import BeautifulSoup\n",
    "from googleapiclient.discovery import build\n",
    "import requests as rs\n",
    "import operator, re\n",
    "import numpy as np\n",
    "from konlpy.tag import Hannanum\n",
    "from google.cloud import translate\n",
    "import os, re, copy\n",
    "from nltk.corpus import wordnet\n",
    "from konlpy.tag import Kkma\n",
    "from celery import shared_task\n",
    "from celery_progress.backend import ProgressRecorder\n",
    "from app.utils import select_company_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_variables():\n",
    "    \"\"\" Initialize global variables \"\"\"\n",
    "    # print(\"1. init variables\")\n",
    "\n",
    "    global file_check, translate_client, tagger, tempre, tempre2\n",
    "\n",
    "    # downloadable file string pattern\n",
    "    file_check = [\"download\", \"down\", \"file\", \"pdf\", \"excel\", \"xlsx\", \"docx\", \"hwp\", \"youtube\"]\n",
    "\n",
    "    # google translate API\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"My Project-a8e42c74ea7e.json\"\n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    # Kkma tagger\n",
    "    tagger = Kkma()\n",
    "\n",
    "    # regex pattern\n",
    "    tempre = re.compile(r'[\\w_]+')\n",
    "    tempre2 = re.compile(r'[\\W_]+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_count_dict():\n",
    "\n",
    "    global company_count_dict\n",
    "    company_count_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_company_list():\n",
    "    \"\"\" Initialize company list variable \"\"\"\n",
    "    # company_list = []\n",
    "    # with open(\"app/static/data/company_list3.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    #     company_list.extend(list(np.unique(f.read().splitlines())))\n",
    "    #\n",
    "    # with open(\"app/static/data/abroad_company_list.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    #     company_list.extend(list(np.unique(f.read().splitlines())))\n",
    "\n",
    "    rows = select_company_db()\n",
    "\n",
    "    company_list = []\n",
    "\n",
    "    for row in rows:\n",
    "        if row[2] != None:\n",
    "            company_list.append(row[2].lower())\n",
    "\n",
    "    return company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getService():\n",
    "    \"\"\" Build an Google Custom Search API service \"\"\"\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=\"AIzaSyCNLAU_Lunh5aJIo17DlslQvKoQGU7yDjA\")\n",
    "\n",
    "    return service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_derived_query(keyword):\n",
    "\n",
    "    # Hannanum pos tagger\n",
    "    hannanum = Hannanum()\n",
    "    \"\"\" Retrieve derived queries from keyword by using WordNet Synset \"\"\"\n",
    "    nouns = [word for word, pos in hannanum.pos(keyword) if pos == \"N\"]\n",
    "    syn_dict = {}\n",
    "    query_list = [keyword]\n",
    "\n",
    "    for noun in nouns:\n",
    "        result = translate_client.translate(noun, target_language=\"en\")\n",
    "        if len(result[\"translatedText\"].split(\" \")) > 1:  # 복합 명사 처리 안함\n",
    "            continue\n",
    "        else:\n",
    "            translated_noun = result[\"translatedText\"]\n",
    "            # print(noun, translated_noun)\n",
    "            for syn in wordnet.synsets(translated_noun):\n",
    "                synonyms = []\n",
    "                if syn.pos() == \"n\":\n",
    "                    syn_word = syn.name().split(\".\")[0]\n",
    "                    synonyms.append(syn_word)\n",
    "\n",
    "        syn_dict[noun] = synonyms\n",
    "\n",
    "    if len(syn_dict) > 0:\n",
    "        for noun in syn_dict:\n",
    "            for syn in syn_dict[noun]:\n",
    "                syn_ko = translate_client.translate(syn, target_language=\"ko\")[\"translatedText\"]\n",
    "                query_list.append(keyword.replace(noun, syn_ko))\n",
    "\n",
    "    return query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_search(keyword, except_words, require_words, condition, syn):\n",
    "    \"\"\" Search urls relevant with keyword by using Google Custom Search API \"\"\"\n",
    "    service = getService()  # GOOGLE API 연결\n",
    "\n",
    "    response_url = []  # URL list\n",
    "\n",
    "    if syn == \"True\":\n",
    "        query_list = get_derived_query(keyword)\n",
    "    elif condition == \"True\":\n",
    "        query_list = keyword\n",
    "    else :\n",
    "        query_list = [keyword]\n",
    "\n",
    "    # print(\"2. retrieve related urls with keyword \", query_list)\n",
    "    for query in query_list:\n",
    "        startIndex = 1  # 시작 인덱스\n",
    "        while (True):\n",
    "            try:\n",
    "                if  condition != \"True\":\n",
    "                    result = service.cse().list(\n",
    "                        q=query,  # 검색 키워드\n",
    "                        cx='001132580745589424302:jbscnf14_dw',  # CSE Key\n",
    "                        lr='lang_ko',  # 검색 언어 (한국어)\n",
    "                        start=startIndex,\n",
    "                        filter=\"0\"\n",
    "                    ).execute()\n",
    "                else :\n",
    "                    result = service.cse().list(\n",
    "                        q=query,  # 검색 키워드\n",
    "                        cx='001132580745589424302:jbscnf14_dw',  # CSE Key\n",
    "                        lr='lang_ko',  # 검색 언어 (한국어)\n",
    "                        start=startIndex,\n",
    "                        filter=\"0\",\n",
    "                        exactTerms=' '.join(require_words),\n",
    "                        excludeTerms=' '.join(except_words)\n",
    "                    ).execute()\n",
    "\n",
    "                # 검색된 결과가 있을 때\n",
    "                if \"items\" in result:\n",
    "                    for item in result[\"items\"]:\n",
    "                        url = item[\"link\"]\n",
    "                        response_url.append(url)\n",
    "\n",
    "                    # INDEX 이동\n",
    "                    if (len(result[\"items\"]) < 10):  # 결과가 10개 미만이면 STOP\n",
    "                        break\n",
    "                    else:  # 결과가 10개면 이동\n",
    "                        startIndex = startIndex + 10\n",
    "                else:\n",
    "                    # print(\"No more Results\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                # print(e)\n",
    "                break\n",
    "\n",
    "    response_url = list(np.unique(response_url))\n",
    "\n",
    "    # print(\"The number of all results : \" + str(len(response_url)))\n",
    "\n",
    "    return response_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_url(url_list):\n",
    "    \"\"\" Extract urls  for exclusion from all url list \"\"\"\n",
    "    # print(\"3. classify page urls\")\n",
    "\n",
    "    download_list = []\n",
    "    html_list = []\n",
    "\n",
    "    for i, url in enumerate(url_list):\n",
    "        # extract downloadable URL including youtube\n",
    "        if (any(ext in url.lower() for ext in file_check)):\n",
    "            download_list.append(url)\n",
    "        else :\n",
    "            html_list.append(url)\n",
    "\n",
    "    # print(\"All : \" + str(len(url_list)) + \" ( HTML URL : \" + str(len(html_list)) + \" / Downloadable URL : \" + str(\n",
    "    #     len(download_list)) + \" )\")\n",
    "    return download_list, html_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locations_of_substring(string, substring):\n",
    "    \"\"\" Return a list of locations of a substring \"\"\"\n",
    "\n",
    "    substring_length = len(substring)\n",
    "    def recurse(locations_found, start):\n",
    "        location = string.find(substring, start)\n",
    "        if location != -1:\n",
    "            return recurse(locations_found + [location], location+substring_length)\n",
    "        else:\n",
    "            return locations_found\n",
    "\n",
    "    return recurse([], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_postposition(word):\n",
    "    if word[-3:] == \"에서는\":\n",
    "        return word[:-3]\n",
    "    elif word[-1] in [\"은\", \"는\", \"이\", \"가\", \"의\"]:\n",
    "        return word[:-1]\n",
    "    elif word[-2:] == \"에서\":\n",
    "        return word[:-2]\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(url):\n",
    "    regex = re.compile('.*footer.*')\n",
    "\n",
    "    footer = \"\"\n",
    "\n",
    "    \"\"\" Parse html and remove unnecessary part of the document \"\"\"\n",
    "    try:\n",
    "        response = rs.get(url)\n",
    "        if response.encoding != None:\n",
    "            html = response.text.encode(response.encoding)\n",
    "        else:\n",
    "            html = response.text\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    if soup == None: return None, None\n",
    "\n",
    "    # remove all script, style\n",
    "    for item in [\"script\", \"style\", \"a\", \"img\"]:\n",
    "        if soup.find(item):\n",
    "            for e in soup.find_all(item):\n",
    "                e.decompose()\n",
    "\n",
    "    # remove div with \"footer\" class\n",
    "    for div in soup.find_all(class_=regex):\n",
    "        footer = div.extract()\n",
    "\n",
    "    if soup.find(id=regex) != None:\n",
    "        if footer != \"\":\n",
    "            footer.append(soup.find(id=regex).extract())\n",
    "        else:\n",
    "            footer = soup.find(id=regex).extract()\n",
    "\n",
    "    # remove \"footer\" element\n",
    "    if soup.find(\"footer\") != None:\n",
    "        if footer != \"\":\n",
    "            footer.append(soup.find(\"footer\").extract())\n",
    "        else:\n",
    "            footer = soup.find(\"footer\").extract()\n",
    "\n",
    "    # extract only text\n",
    "    html_text = soup.get_text().strip().lower()\n",
    "    if footer == \"\":\n",
    "        footer_text = \"\"\n",
    "    else:\n",
    "        footer_text = footer.get_text().strip().lower()\n",
    "\n",
    "    return html_text, footer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_substring(substring, string):\n",
    "    '''\n",
    "    Returns list of indices where substring begins in string\n",
    "\n",
    "    find_substring(\"me\", \"The cat says meow, meow\")\n",
    "    [13, 19]\n",
    "    '''\n",
    "    indices = []\n",
    "    index = -1  # Begins at -1 so index + 1 is 0\n",
    "    while True:\n",
    "        # Find next index of substring, by starting search from index + 1\n",
    "        index = string.find(substring, index + 1)\n",
    "        if index == -1:\n",
    "            break  # All occurrences have been found\n",
    "        indices.append(index)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_condition_expression(query):\n",
    "    exceptre = re.compile('-([ ()and\\u3131-\\u3163\\uac00-\\ud7a3]+)')\n",
    "    requirere = re.compile('\\+([ ()and\\u3131-\\u3163\\uac00-\\ud7a3]+)')\n",
    "    results, except_words, require_words = [], [], []\n",
    "\n",
    "    # 1. \"-\" operator\n",
    "    if query.find(\"-\") != -1:\n",
    "        excepts = exceptre.findall(query)\n",
    "        for e in excepts:\n",
    "            words = e.strip()\n",
    "            if words.find(\"(\") != -1:\n",
    "                words = words.replace(\"(\", \"\").replace(\")\", \"\").strip()\n",
    "                words = re.split(\"and\", words)\n",
    "                for w in words:\n",
    "                    except_words.append(w.strip())\n",
    "            else:\n",
    "                except_words.append(words)\n",
    "\n",
    "    # 2. \"+\" operator\n",
    "    if query.find(\"+\") != -1:\n",
    "        requires = requirere.findall(query)\n",
    "        for r in requires:\n",
    "            words = r.strip()\n",
    "            if words.find(\"(\") != -1:\n",
    "                words = words.replace(\"(\", \"\").replace(\")\", \"\").strip()\n",
    "                words = re.split(\"and\", words)\n",
    "                for w in words:\n",
    "                    require_words.append(w.strip())\n",
    "            else:\n",
    "                require_words.append(words)\n",
    "\n",
    "    query = re.sub(exceptre, \"\", query)\n",
    "    query = re.sub(requirere, \"\", query)\n",
    "\n",
    "    # 3. \"()\"\n",
    "    parenre = re.compile('\\([ andor\\u3131-\\u3163\\uac00-\\ud7a3]+\\)')\n",
    "    parens = parenre.findall(query)\n",
    "    paren_dict = {}\n",
    "    for i, p in enumerate(parens):\n",
    "        query = query.replace(p, str(i))\n",
    "        paren_dict[str(i)] = p\n",
    "\n",
    "    # 4. \"and\", \"or\"\n",
    "    if query.find(\"and\") != -1:\n",
    "        words = re.split(\"and\", query)\n",
    "        for i, w in enumerate(words):\n",
    "            words[i] = w.strip()\n",
    "\n",
    "        if len(paren_dict) != 0:\n",
    "            for k in paren_dict:\n",
    "                if paren_dict[k].find(\"and\") != -1:\n",
    "                    paren_words = []\n",
    "                    for a in paren_dict[k].replace(\"(\", \"\").replace(\")\", \"\").split(\"and\"):\n",
    "                        paren_words.append(a.strip())\n",
    "                    if len(words) - 1 > words.index(k):\n",
    "                        temp = words[words.index(k) + 1:]\n",
    "                        words = words[:words.index(k)] + paren_words + temp\n",
    "                    else:\n",
    "                        words = words[:words.index(k)] + paren_words\n",
    "                    results.append(' '.join(words))\n",
    "                else:\n",
    "                    for a in paren_dict[k].replace(\"(\", \"\").replace(\")\", \"\").split(\"or\"):\n",
    "                        words_bak = copy.deepcopy(words)\n",
    "                        temp = a.strip()\n",
    "                        words_bak[words_bak.index(k)] = temp\n",
    "                        results.append(' '.join(words_bak))\n",
    "        else:\n",
    "            results.append(' '.join(words))\n",
    "\n",
    "    return results, except_words, require_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"드론\"\n",
    "condition = \"False\"\n",
    "syn = \"False\"\n",
    "except_words, require_words = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_variables()\n",
    "init_count_dict()\n",
    "company_list = init_company_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [\"은\", \"는\", \"이\", \"가\", \"의\", \"와\", \"과\"]\n",
    "B = \"에서\"\n",
    "C = \"입니다\"\n",
    "execpt_company = [\"(주)메커니즘\", \"(주)스위스\", \"(주)현대인\", \"(주)플러스\", \"우리나라(주)\", \"혈당측정기(주)\", \"당뇨환자(주)\"]\n",
    "news_keywords = [\"news\", \"뉴스\", \"신문\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_list = [\"https://www.anadronestarting.com/glossary/parrot/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_list = [\"(주)parrot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL 0. https://www.anadronestarting.com/glossary/parrot/\n"
     ]
    }
   ],
   "source": [
    "for i, url in enumerate(html_list):\n",
    "    \n",
    "    print(\"URL\", str(i) + \". \" + url)\n",
    "    \n",
    "    html_text, footer_text = parse_html(url)\n",
    "    \n",
    "    if html_text == None : continue \n",
    "        \n",
    "    found_index=list = []\n",
    "    for company in company_list : \n",
    "        \n",
    "        found = False \n",
    "        \n",
    "        company3 = company.replace(\"(주)\", \"㈜\")\n",
    "\n",
    "        if (company in html_text) | (company3 in html_text):  # 본문에서 검색\n",
    "\n",
    "            # 특수문자 \"㈜\" 로 대체\n",
    "            if company not in html_text:\n",
    "                company = company3\n",
    "\n",
    "            temp_idx = html_text.find(company)\n",
    "            # 기업명 뒤가 주격조사, 특수문자, 공백 중 하나로 끝나는지 체크\n",
    "            if (html_text[temp_idx + len(company)] in A) | \\\n",
    "                    (html_text[temp_idx + len(company): temp_idx + len(company) + 2] == B) | \\\n",
    "                    (html_text[temp_idx + len(company): temp_idx + len(company) + 3] == C):\n",
    "                found = True\n",
    "                company = company.replace(\"㈜\", \"(주)\")\n",
    "                print(\"Case 1-A)\", company)\n",
    "                if company in company_count_dict:\n",
    "                    company_count_dict[company][\"count\"] += 1\n",
    "                    company_count_dict[company][\"url_list\"].append(url)\n",
    "                else:\n",
    "                    company_count_dict[company] = {\"count\": 1, \"url_list\": [url]}\n",
    "                found_index_list.append(temp_idx)\n",
    "            else:\n",
    "                pass\n",
    "                # print(company, \" is real company name?\")\n",
    "\n",
    "        elif (company in footer_text) | (company3 in footer_text):  # footer에서 검색\n",
    "            if any(n in footer_text for n in news_keywords): continue\n",
    "\n",
    "            # 특수문자 \"㈜\"로 대체\n",
    "            if company not in footer_text:\n",
    "                company = company3\n",
    "\n",
    "            temp_idx = footer_text.find(company)\n",
    "\n",
    "            if (footer_text[temp_idx + len(company)] in A) | \\\n",
    "                    (footer_text[temp_idx + len(company): temp_idx + len(company) + 2] == B) | \\\n",
    "                    (footer_text[temp_idx + len(company): temp_idx + len(company) + 3] == C):\n",
    "                found = True\n",
    "                company = company.replace(\"㈜\", \"(주)\")\n",
    "                print(\"Case 1-B)\", company)\n",
    "                if company in company_count_dict:\n",
    "                    company_count_dict[company][\"count\"] += 1\n",
    "                    company_count_dict[company][\"url_list\"].append(url)\n",
    "                else:\n",
    "                    company_count_dict[company] = {\"count\": 1, \"url_list\": [url]}\n",
    "\n",
    "        '''\n",
    "        Case 2) 기업 DB 사용, \"(주)\" 포함하지 않음\n",
    "        '''\n",
    "\n",
    "        if (found == True) | (company in execpt_company): continue\n",
    "\n",
    "        company2 = company.replace(\"(주)\", \"\").replace(\"㈜\", \"\").strip()\n",
    "\n",
    "        if len(company2) <= 2: continue  # 기업명 변수 길이 제한\n",
    "\n",
    "        if company2 in html_text:\n",
    "            occur_indices = find_substring(company2, html_text)\n",
    "            occur_indices_f = []\n",
    "            for idx in occur_indices:\n",
    "                if idx + len(company2) == len(html_text):\n",
    "                    if html_text[idx - 1] in [\" \", \"\\n\", \"(\", \"\\r\", \"\\t\"]:\n",
    "                        # find both start and end of the sentence\n",
    "                        start = len(html_text[:idx]) - [m.start(0) for m in re.finditer(tempre2, html_text[:idx][::-1])][0]\n",
    "                        temp_sentence = html_text[start:].strip()\n",
    "                        if company2 in tagger.nouns(temp_sentence):\n",
    "                            found = True\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                elif (html_text[idx - 1] in [\" \", \"\\n\", \"(\", \"\\r\", \"\\t\"]) & (\n",
    "                        (html_text[idx + len(company2)] in A) | (len(tempre2.findall(html_text[idx + len(company2)])) != 0) | (\n",
    "                        html_text[idx + len(company2):idx + len(company2) + 2] == B) | (\n",
    "                    html_text[idx + len(company2): idx + len(company2) + 3] == C)):\n",
    "                    start = len(html_text[:idx]) - [m.start(0) for m in re.finditer(tempre2, html_text[:idx][::-1])][0]\n",
    "                    end = idx + [m.start(0) for m in re.finditer(tempre2, html_text[idx:])][0]\n",
    "                    temp_sentence = html_text[start:end].strip()\n",
    "                    if company2 in tagger.nouns(temp_sentence):\n",
    "                        found = True\n",
    "                        break\n",
    "\n",
    "            if found == True:\n",
    "                print(\"Case 2)\", company)\n",
    "                if company in company_count_dict:\n",
    "                    company_count_dict[company][\"count\"] += 1\n",
    "                    company_count_dict[company][\"url_list\"].append(url)\n",
    "                else:\n",
    "                    company_count_dict[company] = {\"count\": 1, \"url_list\": [url]}\n",
    "            else:\n",
    "                pass\n",
    "                # print(company, \" is real company name? 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[264]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occur_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = occur_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parrot.com\\n프랑스의  제조업체입니다.\\n‘’라는 비행 애플리케이션을 활용한 쉬운 조종과\\n귀여운 디자인을 무기로 많은 팬을 확보하고 있습니다.\\n대표 제품으로는 비밥 (Bebop Drone), (Rolling Spider), AR.Drone 등이 있습니다.\\n\\n\\n\\n\\n \\n이 글을 읽은 분들이 선택한 기사2018년 8월 07일2018년 8월 07일2018년 8월 06일2018년 8월 06일2018년 8월 03일2018년 8월 03일'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_text[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(주)패럿'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
